{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "英文分词相关API如下，nltk将会寻找punkt资源\n",
    "~/nltk_data/tokenizers/punkt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tokenize as tk\n",
    "# 将样本按句子进行拆分  sent_list：句子列表\n",
    "sent_list = tk.sent_tokenize(text='text')\n",
    "\n",
    "# 将样本按单词进行拆分  word_list：单词列表\n",
    "word_list = tk.word_tokenize(text='text')\n",
    "\n",
    "# 将样本按单词进行拆分  punctTokenizer：分词器对象\n",
    "punctTokenizer = tk.WordPunctTokenizer()\n",
    "word_list = punctTokenizer.tokenize(text='text')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**案例：英文分词**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk.tokenize as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\n"
     ]
    }
   ],
   "source": [
    "doc = \"Are you curious about tokenization? \" \\\n",
    "\"Let's see how it works! \" \\\n",
    "\"We need to analyze a couple of sentences \" \\\n",
    "\"with punctuations to see it in action.\"\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : Are you curious about tokenization?\n",
      "2 : Let's see how it works!\n",
      "3 : We need to analyze a couple of sentences with punctuations to see it in action.\n"
     ]
    }
   ],
   "source": [
    "# 分句子\n",
    "sents = tk.sent_tokenize(doc)\n",
    "for i in range(len(sents)):\n",
    "    print(i+1,':',sents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : Are\n",
      "2 : you\n",
      "3 : curious\n",
      "4 : about\n",
      "5 : tokenization\n",
      "6 : ?\n",
      "7 : Let\n",
      "8 : 's\n",
      "9 : see\n",
      "10 : how\n",
      "11 : it\n",
      "12 : works\n",
      "13 : !\n",
      "14 : We\n",
      "15 : need\n",
      "16 : to\n",
      "17 : analyze\n",
      "18 : a\n",
      "19 : couple\n",
      "20 : of\n",
      "21 : sentences\n",
      "22 : with\n",
      "23 : punctuations\n",
      "24 : to\n",
      "25 : see\n",
      "26 : it\n",
      "27 : in\n",
      "28 : action\n",
      "29 : .\n"
     ]
    }
   ],
   "source": [
    "# 分单词-01\n",
    "words = tk.word_tokenize(doc)\n",
    "for i in range(len(words)):\n",
    "    print(i+1,':',words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : Are\n",
      "2 : you\n",
      "3 : curious\n",
      "4 : about\n",
      "5 : tokenization\n",
      "6 : ?\n",
      "7 : Let\n",
      "8 : '\n",
      "9 : s\n",
      "10 : see\n",
      "11 : how\n",
      "12 : it\n",
      "13 : works\n",
      "14 : !\n",
      "15 : We\n",
      "16 : need\n",
      "17 : to\n",
      "18 : analyze\n",
      "19 : a\n",
      "20 : couple\n",
      "21 : of\n",
      "22 : sentences\n",
      "23 : with\n",
      "24 : punctuations\n",
      "25 : to\n",
      "26 : see\n",
      "27 : it\n",
      "28 : in\n",
      "29 : action\n",
      "30 : .\n"
     ]
    }
   ],
   "source": [
    "# 分单词-02\n",
    "tokenizer = tk.WordPunctTokenizer()\n",
    "words = tokenizer.tokenize(doc)\n",
    "for i in range(len(words)):\n",
    "    print(i+1,':',words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 16)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 19)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 17)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 15)\t2\n",
      "  (2, 2)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 18)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 1)\t1\n",
      "[[1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1]\n",
      " [0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0]\n",
      " [0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 2 0 1 1 0 0]]\n",
      "['about' 'action' 'analyze' 'are' 'couple' 'curious' 'how' 'in' 'it' 'let'\n",
      " 'need' 'of' 'punctuations' 'see' 'sentences' 'to' 'tokenization' 'we'\n",
      " 'with' 'works' 'you']\n"
     ]
    }
   ],
   "source": [
    "import sklearn.feature_extraction.text as ft \n",
    "# 构建词袋模型对象\n",
    "cv = ft.CountVectorizer()\n",
    "# 训练模型，把句子中所有可能出现的单词作为特征名，\n",
    "# 每一个句子为一个样本，单词在句子中出现的次数作为特征值\n",
    "# sentences:[] 将句子中可能出现的单词放入列表中\n",
    "bow = cv.fit_transform(sents)\n",
    "print(bow)\n",
    "print(bow.toarray())\n",
    "print(cv.get_feature_names_out())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**词频逆文档频率 TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45 0.   0.   0.45 0.   0.45 0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.45 0.   0.   0.   0.45]\n",
      " [0.   0.   0.   0.   0.   0.   0.49 0.   0.37 0.49 0.   0.   0.   0.37\n",
      "  0.   0.   0.   0.   0.   0.49 0.  ]\n",
      " [0.   0.26 0.26 0.   0.26 0.   0.   0.26 0.2  0.   0.26 0.26 0.26 0.2\n",
      "  0.26 0.51 0.   0.26 0.26 0.   0.  ]]\n",
      "['about' 'action' 'analyze' 'are' 'couple' 'curious' 'how' 'in' 'it' 'let'\n",
      " 'need' 'of' 'punctuations' 'see' 'sentences' 'to' 'tokenization' 'we'\n",
      " 'with' 'works' 'you']\n"
     ]
    }
   ],
   "source": [
    "tt = ft.TfidfTransformer()\n",
    "tfidf = tt.fit_transform(bow).toarray()\n",
    "print(np.round(tfidf,2))\n",
    "print(cv.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd16a1c2e981052eaae61151b9525ae9913f1f0d16bca6b7e7be9e0f29d739d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
